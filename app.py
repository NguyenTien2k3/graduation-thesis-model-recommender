import pickle
import pandas as pd
from flask import Flask, request, jsonify
import os
import logging
import requests
import time
import threading
import gc # Import Garbage Collector ƒë·ªÉ qu·∫£n l√Ω RAM

# ==========================================================================
# 1. C·∫•u h√¨nh & H·∫±ng s·ªë
# ==========================================================================
# Thi·∫øt l·∫≠p c·∫•u h√¨nh log c∆° b·∫£n
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

app = Flask(__name__)

# C√ÅC URL V√Ä PATH C·ª¶A FILE D·ªÆ LI·ªÜU
MODEL_HF_URL = "https://huggingface.co/Stas2k3/svd_model_nf32_lr0.001_reg0.05_ep40_p1.0_balanced/resolve/main/svd_model_nf32_lr0.001_reg0.05_ep40_p1.0_balanced.pkl"
CSV_HF_URL = "https://huggingface.co/datasets/Stas2k3/Cell_Phones_and_Accessories_Train/resolve/main/Cell_Phones_and_Accessories.train.csv"

# URL D·ªÆ LI·ªÜU ƒê√É T√çNH TO√ÅN TR∆Ø·ªöC (R·∫§T QUAN TR·ªåNG CHO T·ªêC ƒê·ªò API)
# B·∫†N C·∫¶N THAY TH·∫æ B·∫∞NG URL D·∫™N ƒê·∫æN FILE PICKLE CH·ªà CH·ª®A TOP N G·ª¢I √ù CHO M·ªñI USER
PRECOMPUTED_HF_URL = "YOUR_PRECOMPUTED_TOP_K_URL_HERE" 

# ƒê∆∞·ªùng d·∫´n cache t·∫°m
CACHE_DIR = "/tmp"
MODEL_PATH = os.path.join(CACHE_DIR, "model.pkl")
CSV_PATH = os.path.join(CACHE_DIR, "data.csv")
ITEM_IDS_PATH = os.path.join(CACHE_DIR, "item_ids.pkl")
PRECOMPUTED_PATH = os.path.join(CACHE_DIR, "precomputed_recs.pkl")

# Kh·ªüi t·∫°o d·ªØ li·ªáu to√†n c·ª•c (s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t b·ªüi lu·ªìng n·ªÅn)
item_ids = []
topk_model = None
precomputed_recommendations = {} # D·ªØ li·ªáu g·ª£i √Ω ƒë√£ t√≠nh to√°n tr∆∞·ªõc

# ==========================================================================
# 2. H√†m t·∫£i file t·ªëi ∆∞u RAM (stream)
# ==========================================================================

def download_file_stream(url, save_path, name, max_retries=5):
    """T·∫£i file theo lu·ªìng (stream) ƒë·ªÉ tr√°nh t·ªën RAM v√† gi·∫£m t·∫ßn su·∫•t ghi log."""
    if os.path.exists(save_path):
        logging.info(f"[{name}] File ƒë√£ t·ªìn t·∫°i trong cache: {save_path}")
        return True
    
    # B·ªè qua n·∫øu URL l√† placeholder
    if url == "YOUR_PRECOMPUTED_TOP_K_URL_HERE":
        logging.warning(f"[{name}] URL ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. B·ªè qua t·∫£i.")
        return False

    for attempt in range(max_retries):
        try:
            logging.info(f"[{name}] T·∫£i t·ª´ {url} (attempt {attempt + 1})...")
            last_reported_percent = -1
            with requests.get(url, stream=True, timeout=60) as r:
                r.raise_for_status()
                total_size = int(r.headers.get("content-length", 0))
                downloaded = 0
                with open(save_path, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            if total_size:
                                percent = downloaded / total_size * 100
                                current_major_percent = int(percent // 20) * 20
                                if current_major_percent > last_reported_percent and current_major_percent < 100:
                                    logging.info(f"[{name}] {current_major_percent}% downloaded")
                                    last_reported_percent = current_major_percent

            logging.info(f"[{name}] ‚úÖ Ho√†n t·∫•t t·∫£i: {save_path}")
            return True
        except Exception as e:
            logging.warning(f"[{name}] L·ªói khi t·∫£i: {e}")
            if attempt < max_retries - 1:
                wait = 2 ** (attempt + 1)
                logging.info(f"[{name}] ƒê·ª£i {wait}s tr∆∞·ªõc khi retry...")
                time.sleep(wait)
    logging.error(f"[{name}] ‚ùå T·∫£i th·∫•t b·∫°i sau {max_retries} l·∫ßn.")
    return False

# ==========================================================================
# 3. Load d·ªØ li·ªáu (T·ªëi ∆∞u RAM)
# ==========================================================================

def _load_items_blocking():
    """T·∫£i v√† load danh s√°ch item ID (∆Øu ti√™n load t·ª´ cache nhanh ITEM_IDS_PATH)."""
    if os.path.exists(ITEM_IDS_PATH):
        try:
            logging.info("ƒêang load Item IDs t·ª´ cache nhanh...")
            with open(ITEM_IDS_PATH, 'rb') as f:
                unique_items = pickle.load(f)
            logging.info(f"‚úÖ Item IDs ƒë√£ load t·ª´ cache: {len(unique_items)} items duy nh·∫•t.")
            return unique_items
        except Exception as e:
            logging.warning(f"L·ªói ƒë·ªçc cache Item IDs: {e}. S·∫Ω load l·∫°i t·ª´ CSV.")

    if not download_file_stream(CSV_HF_URL, CSV_PATH, "CSV Data"):
        return []
    
    try:
        logging.info("ƒê·ªçc CSV T·ª™ ƒêƒ®A (qu√° tr√¨nh ch·∫≠m)...")
        # CH·ªà T·∫¢I C√ÅC C·ªòT C·∫¶N THI·∫æT (parent_asin) ƒê·ªÇ GI·∫¢M RAM
        items_df = pd.read_csv(CSV_PATH, usecols=["parent_asin"])
        
        # X·ª≠ l√Ω c·ªôt parent_asin
        items_df["parent_asin"] = (
            items_df["parent_asin"].astype(str).str.split(",").str[0]
        )
        unique_items = items_df["parent_asin"].dropna().unique().tolist()
        logging.info(f"‚úÖ CSV ƒë√£ load v√† x·ª≠ l√Ω: {len(unique_items)} items duy nh·∫•t.")

        # D·ªåN D·∫∏P RAM SAU KHI S·ª¨ D·ª§NG DATAFRAME L·ªöN
        del items_df
        gc.collect() 
        logging.info("B·ªô nh·ªõ ƒë√£ ƒë∆∞·ª£c d·ªçn d·∫πp sau khi x·ª≠ l√Ω CSV.")

        # L∆∞u k·∫øt qu·∫£ x·ª≠ l√Ω v√†o cache nhanh ITEM_IDS_PATH
        try:
            with open(ITEM_IDS_PATH, 'wb') as f:
                pickle.dump(unique_items, f)
            logging.info("‚úÖ ƒê√£ t·∫°o cache nhanh Item IDs.")
        except Exception as e:
            logging.warning(f"L·ªói khi l∆∞u cache Item IDs: {e}")

        return unique_items
    except Exception as e:
        logging.error(f"L·ªói ƒë·ªçc CSV: {e}")
        return []

def _load_model_blocking():
    """T·∫£i model pickle (SVD) b·∫±ng stream (blocking operation)."""
    if not download_file_stream(MODEL_HF_URL, MODEL_PATH, "Model"):
        return None
    try:
        logging.info("ƒêang load Model t·ª´ ƒëƒ©a (qu√° tr√¨nh ch·∫≠m v√† t·ªën RAM)...")
        with open(MODEL_PATH, "rb") as f:
            model = pickle.load(f)
        logging.info("‚úÖ Model ƒë√£ load th√†nh c√¥ng.")
        return model
    except Exception as e:
        logging.error(f"L·ªói load model: {e}")
        return None

def _load_precomputed_data():
    """T·∫£i v√† load d·ªØ li·ªáu g·ª£i √Ω ƒë√£ t√≠nh to√°n tr∆∞·ªõc (T·ªêC ƒê·ªò CAO)."""
    if not download_file_stream(PRECOMPUTED_HF_URL, PRECOMPUTED_PATH, "Precomputed Data"):
        return {}
    
    try:
        logging.info("ƒêang load D·ªØ li·ªáu g·ª£i √Ω ƒë√£ t√≠nh to√°n tr∆∞·ªõc...")
        with open(PRECOMPUTED_PATH, "rb") as f:
            data = pickle.load(f)
        logging.info(f"‚úÖ D·ªØ li·ªáu Pre-computed ƒë√£ load th√†nh c√¥ng. (S·ªë l∆∞·ª£ng users: {len(data)})")
        return data
    except Exception as e:
        logging.error(f"L·ªói load d·ªØ li·ªáu Pre-computed: {e}")
        return {}

# ==========================================================================
# 4. Kh·ªüi ƒë·ªông d·ªØ li·ªáu b·∫•t ƒë·ªìng b·ªô (ASYNC)
# ==========================================================================

def load_data_and_model_async():
    """H√†m m·ª•c ti√™u cho lu·ªìng n·ªÅn, ch·ªãu tr√°ch nhi·ªám load d·ªØ li·ªáu v√† model n·∫∑ng."""
    global item_ids, topk_model, precomputed_recommendations
    logging.info("üöÄ Lu·ªìng n·ªÅn: B·∫Øt ƒë·∫ßu t·∫£i Item IDs, Model v√† D·ªØ li·ªáu Pre-computed...")
    
    # 1. T·∫£i Item IDs (S·ª≠ d·ª•ng cache nhanh)
    item_ids = _load_items_blocking()
    
    # 2. T·∫£i Model (Ph·∫ßn ch·∫≠m, t·ªën RAM)
    topk_model = _load_model_blocking()
    
    # 3. T·∫£i D·ªØ li·ªáu Pre-computed (Quan tr·ªçng cho t·ªëc ƒë·ªô g·ª£i √Ω)
    precomputed_recommendations = _load_precomputed_data()
    
    logging.info("‚úÖ Lu·ªìng n·ªÅn: Ho√†n t·∫•t t·∫•t c·∫£ qu√° tr√¨nh load d·ªØ li·ªáu.")


# ==========================================================================
# 5. H√†m g·ª£i √Ω (S·ª≠ d·ª•ng Cache)
# ==========================================================================

def get_top_k_recommendations(user_id, k=10, blocked_items=None):
    """
    H√†m g·ª£i √Ω hi·ªáu su·∫•t cao: Ch·ªâ tra c·ª©u trong d·ªØ li·ªáu ƒë√£ t√≠nh to√°n tr∆∞·ªõc.
    Lo·∫°i b·ªè v√≤ng l·∫∑p 95,000 ph√©p t√≠nh ch·∫≠m ch·∫°p.
    """
    # 1. Ki·ªÉm tra D·ªØ li·ªáu Pre-computed
    if not precomputed_recommendations:
        # N·∫øu ch∆∞a load ƒë∆∞·ª£c Pre-compute, fallback v·ªÅ Model SVD (CH·∫¨M)
        if topk_model:
            logging.warning("S·ª≠ d·ª•ng fallback g·ª£i √Ω SVD (R·∫§T CH·∫¨M).")
            # N·∫øu ng∆∞·ªùi d√πng ƒë√£ cung c·∫•p URL Pre-computed, nh∆∞ng n√≥ l·ªói, 
            # ch√∫ng ta kh√¥ng n√™n ch·∫°y 95k ph√©p t√≠nh ·ªü ƒë√¢y. 
            # Gi·∫£ s·ª≠ ch√∫ng ta CH·ªà d·ª±a v√†o Pre-computed.
            return [{"item_id": "fallback_error", "predicted_rating": 0.0, "note": "Precomputed data missing, cannot suggest."}]
        else:
            return [{"error": "Model and Precomputed data not loaded"}]

    user_str = str(user_id)
    
    # 2. Tra c·ª©u trong Cache Pre-computed
    if user_str not in precomputed_recommendations:
        logging.warning(f"User {user_id} kh√¥ng c√≥ trong cache Pre-computed.")
        # N·∫øu user m·ªõi, c√≥ th·ªÉ fallback v·ªÅ g·ª£i √Ω Top Ph·ªï Bi·∫øn (ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t)
        return [{"error": "User not found in precomputed cache"}]

    # L·∫•y danh s√°ch g·ª£i √Ω ƒë√£ s·∫Øp x·∫øp cho user
    all_recs = precomputed_recommendations[user_str]
    
    # 3. √Åp d·ª•ng Blocked Items v√† gi·ªõi h·∫°n Top K
    blocked_set = set(blocked_items or [])
    final_recs = []
    
    for rec in all_recs:
        if rec['item_id'] not in blocked_set:
            final_recs.append(rec)
            if len(final_recs) >= k:
                break
                
    return final_recs


# ==========================================================================
# 6. API Endpoint
# ==========================================================================

@app.route("/recommend", methods=["POST"])
def recommend():
    # Ki·ªÉm tra tr·∫°ng th√°i d·ªØ li·ªáu Pre-computed tr∆∞·ªõc khi x·ª≠ l√Ω request
    if not precomputed_recommendations:
        logging.warning("Y√™u c·∫ßu g·ª£i √Ω th·∫•t b·∫°i: D·ªØ li·ªáu Pre-computed ƒëang ƒë∆∞·ª£c t·∫£i.")
        # Tr·∫£ v·ªÅ l·ªói 503 (Service Unavailable) n·∫øu d·ªØ li·ªáu g·ª£i √Ω t·ªëc ƒë·ªô cao ch∆∞a load xong
        return jsonify({"error": "Precomputed recommendations are still loading (Model loading). Please wait a few seconds."}), 503
    
    try:
        data = request.get_json(force=True)
    except Exception as e:
        return jsonify({"error": f"Invalid JSON payload: {e}"}), 400

    user_id = data.get("user_id")
    k = int(data.get("top_k", 10))
    blocked_items = data.get("blocked_items", [])

    if not user_id:
        return jsonify({"error": "user_id is required"}), 400

    start_time = time.time()
    recommendations = get_top_k_recommendations(user_id, k, blocked_items)
    end_time = time.time()

    logging.info(f"‚úÖ G·ª£i √Ω cho user {user_id} ho√†n t·∫•t trong {(end_time - start_time) * 1000:.2f}ms")

    # Th√™m ki·ªÉm tra l·ªói cu·ªëi c√πng
    if recommendations and "error" in recommendations[0]:
        return jsonify({"error": recommendations[0]["error"]}), 500

    return jsonify(recommendations), 200

@app.route("/health", methods=["GET"])
def health():
    # Health check ph·∫£n √°nh tr·∫°ng th√°i c·ªßa model v√† d·ªØ li·ªáu g·ª£i √Ω (t·ª©c l√† ƒë√£ load xong ch∆∞a)
    return jsonify({
        "status": "healthy",
        # Tr·∫£ v·ªÅ false trong qu√° tr√¨nh t·∫£i model v√† d·ªØ li·ªáu pre-computed
        "model_loaded": topk_model is not None, 
        "precomputed_loaded": bool(precomputed_recommendations),
        "items_count": len(item_ids)
    }), 200

# ==========================================================================
# 7. Kh·ªüi ƒë·ªông Gunicorn/WSGI (Thread Initialization)
# ==========================================================================
# B·∫Øt ƒë·∫ßu lu·ªìng n·ªÅn ƒë·ªÉ t·∫£i d·ªØ li·ªáu v√† model.
# ƒêo·∫°n code n√†y ch·∫°y khi module ƒë∆∞·ª£c import b·ªüi Gunicorn/WSGI worker.
logging.info("üöÄ B·∫Øt ƒë·∫ßu lu·ªìng n·ªÅn t·∫£i d·ªØ li·ªáu...")
threading.Thread(target=load_data_and_model_async, daemon=True).start()

# H√†m main ch·ªâ d√†nh cho ph√°t tri·ªÉn c·ª•c b·ªô (local development)
if __name__ == "__main__":
    logging.info("‚úÖ Ch·∫°y ch·∫ø ƒë·ªô ph√°t tri·ªÉn c·ª•c b·ªô (local development)...")
    port = int(os.environ.get("PORT", 8000))
    app.run(host="0.0.0.0", port=port)
